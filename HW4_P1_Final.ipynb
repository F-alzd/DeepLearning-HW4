{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbIUd6pCFuCk",
        "outputId": "ddf8f7db-8453-4b8c-aa00-240079e6d22e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.25.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (4.0.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive/')\n",
        "!pip install --upgrade --no-cache-dir gdown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BP-5Jrudn73Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3c1f466-3b6e-41e3-8d5b-a47d3fa0bf1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.12.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.8/dist-packages (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from sacrebleu) (1.21.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.8/dist-packages (from sacrebleu) (0.8.10)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from sacrebleu) (4.9.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.8/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.8/dist-packages (from sacrebleu) (2.7.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from sacrebleu) (2022.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install sacrebleu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xAAMKtbEFuCo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a59ff3d-60be-4bbe-a268-3902b89a6058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import datasets\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import gdown\n",
        "from torchvision.transforms import ToTensor, Resize\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms import ToTensor, Resize, Compose\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import seaborn as sb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "import PIL \n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ferdousi_path = \"/content/drive/MyDrive/ferdousi.txt\"\n",
        "with open(ferdousi_path, \"r\") as file1:\n",
        "    ferdousi = file1.read()"
      ],
      "metadata": {
        "id": "CoTJavJfGVOE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "lUmrOFySJHg6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqCLrrSHmaTj",
        "outputId": "207e8032-0503-4203-f428-7686c5b360e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://github.com/Text-Mining/Persian-Wikipedia-Corpus/raw/master/models/glove/vectors.zip\n",
            "To: /content/vectors.zip\n",
            "100%|██████████| 48.2M/48.2M [00:00<00:00, 294MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  vectors.zip\n",
            "replace vectors.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: vectors.txt             \n"
          ]
        }
      ],
      "source": [
        "gdown.download(url=\"https://github.com/Text-Mining/Persian-Wikipedia-Corpus/raw/master/models/glove/vectors.zip\",output=\"vectors.zip\")\n",
        "!unzip \"vectors.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXUYdGpMFuCq"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "R9CGMKbXFuCs"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "MAX_LENGTH = 0\n",
        "PAD_token = 2\n",
        "\n",
        "#initialize Lang Class\n",
        "class Lang:\n",
        "   def __init__(self):\n",
        "       \n",
        "       self.word2index = {}\n",
        "       self.word2count = {}\n",
        "       self.index2word = {0: \"SOS\", 1: \"EOS\", 2:\"PAD\"} \n",
        "       self.n_words = 3  \n",
        "\n",
        "   def addSentence(self, sentence):\n",
        "       for word in sentence.split(' '):\n",
        "           self.addWord(word)\n",
        "\n",
        "   def addWord(self, word):\n",
        "       if word not in self.word2index:\n",
        "           self.word2index[word] = self.n_words\n",
        "           self.word2count[word] = 1\n",
        "           self.index2word[self.n_words] = word\n",
        "           self.n_words += 1\n",
        "       else:\n",
        "           self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "S6F9ZwDiFuCt"
      },
      "outputs": [],
      "source": [
        "def read_sentence(df, lang1, lang2):\n",
        "   sentence1 = df[lang1]\n",
        "   sentence2 = df[lang2]\n",
        "   return sentence1, sentence2\n",
        "   \n",
        "def Word2vec(path=\"/content/vectors.txt\"):\n",
        "  file=open(path, mode=\"r\")\n",
        "  L=file.readlines(-1)\n",
        "  Word_feature={}\n",
        "  for i in L:\n",
        "    vect=i.split()\n",
        "    Word_feature[vect[0]]=np.array(vect[1:],dtype=float)\n",
        "  return Word_feature\n",
        "\n",
        "def read_file(loc, lang1, lang2):\n",
        "   df = pd.read_csv(loc, delimiter='\\n', header=None, names=['Beit'])[2::]\n",
        "   d = {lang1:df[0::2].values[:,0].tolist() , lang2: df[1::2].values[:,0].tolist()}\n",
        "   df=pd.DataFrame(d)\n",
        "   return df\n",
        "\n",
        "def process_data(lang1,lang2):\n",
        "   global MAX_LENGTH\n",
        "   df = read_file('/content/drive/MyDrive/ferdousi.txt', lang1, lang2)\n",
        "   print(\"Read %s sentence pairs\" % len(df))\n",
        "   sentence1, sentence2 = read_sentence(df, lang1, lang2)\n",
        "\n",
        "   source = Lang()\n",
        "   target = Lang()\n",
        "   pairs = []\n",
        "\n",
        "   for i in range(len(df)):\n",
        "      if len(sentence1[i].split(' '))>MAX_LENGTH:\n",
        "         MAX_LENGTH = len(sentence1[i].split(' '))\n",
        "     \n",
        "      full = [sentence1[i], sentence2[i]]\n",
        "      source.addSentence(sentence1[i])\n",
        "      target.addSentence(sentence2[i])\n",
        "      pairs.append(full)\n",
        "\n",
        "   print(f\"Maximum lenght of input is {MAX_LENGTH}\")\n",
        "\n",
        "   return source, target, pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_mlMEoMFuCu",
        "outputId": "bdb78211-b9aa-4fac-c631-c04afcd487ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 49608 sentence pairs\n",
            "Maximum lenght of input is 11\n"
          ]
        }
      ],
      "source": [
        "lang1 = 'M1'\n",
        "lang2 = 'M2'\n",
        "a = read_file('/content/drive/MyDrive/ferdousi.txt', lang1, lang2)\n",
        "source, target, pairs = process_data(lang1, lang2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "I6XufbzrFuCw"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "   return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "   global PAD_token\n",
        "   global MAX_LENGTH\n",
        "   indexes = indexesFromSentence(lang, sentence)\n",
        "   pad_list=[PAD_token for i in range(MAX_LENGTH-len(indexes))]\n",
        "   indexes =  indexes  + pad_list\n",
        "   indexes.append(EOS_token)\n",
        "   return torch.tensor(indexes, dtype=torch.long,device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(input_lang, output_lang, pair):\n",
        "   input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "   target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "   return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmnrWQT9FuCy"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bCQ5JSxqFuCy"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "   def __init__(self, input_dim, hidden_dim, embbed_dim, num_layers,GRU=False):\n",
        "       super(Encoder, self).__init__()\n",
        "      \n",
        "       #set the encoder input dimesion , embbed dimesion, hidden dimesion, and number of layers \n",
        "       self.input_dim = input_dim\n",
        "       self.embbed_dim = embbed_dim\n",
        "       self.hidden_dim = hidden_dim\n",
        "       self.num_layers = num_layers\n",
        "\n",
        "       #initialize the embedding layer with input and embbed dimention\n",
        "       self.embedding = nn.Embedding(input_dim, self.embbed_dim,padding_idx=2) #\n",
        "       #intialize the GRU to take the input dimetion of embbed, and output dimention of hidden and\n",
        "       #set the number of gru layers\n",
        "       if GRU:\n",
        "        self.recurrent = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
        "       else:\n",
        "        self.recurrent = nn.LSTM(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
        "              \n",
        "   def forward(self, src):\n",
        "       \n",
        "       embedded = self.embedding(src).view(1,1,-1)\n",
        "       outputs, hidden = self.recurrent(embedded)\n",
        "       return outputs, hidden\n",
        "   def initHidden(self):\n",
        "     return torch.zeros(1, 1, self.hidden_dim, device=device)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "   def __init__(self, output_dim, hidden_dim, embbed_dim, num_layers,GRU=False):\n",
        "       super(Decoder, self).__init__()\n",
        "\n",
        "#set the encoder output dimension, embed dimension, hidden dimension, and number of layers \n",
        "       self.embbed_dim = embbed_dim\n",
        "       self.hidden_dim = hidden_dim\n",
        "       self.output_dim = output_dim\n",
        "       self.num_layers = num_layers\n",
        "\n",
        "# initialize every layer with the appropriate dimension. For the decoder layer, it will consist of an embedding, GRU, a Linear layer and a Log softmax activation function.\n",
        "       self.embedding = nn.Embedding(output_dim, self.embbed_dim ,padding_idx=2) # \n",
        "       \n",
        "       if GRU:\n",
        "        self.recurrent = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
        "       else :\n",
        "        self.recurrent=nn.LSTM(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
        "\n",
        "       self.out = nn.Linear(self.hidden_dim, output_dim)\n",
        "       self.softmax = nn.LogSoftmax(dim=1)\n",
        "      \n",
        "   def forward(self, input, hidden):\n",
        "\n",
        "# reshape the input to (1, batch_size)\n",
        "       input = input.view(1, -1)\n",
        "       embedded = F.relu(self.embedding(input))\n",
        "       output, hidden = self.recurrent(embedded, hidden)       \n",
        "       prediction = self.softmax(self.out(output[0]))\n",
        "      \n",
        "       return prediction, hidden\n",
        "   def initHidden(self):\n",
        "      return torch.zeros(1, 1, self.hidden_dim, device=device)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "   def __init__(self, encoder, decoder, device, MAX_LENGTH=MAX_LENGTH,mode='GRU'):\n",
        "       super().__init__()\n",
        "      \n",
        "#initialize the encoder and decoder\n",
        "       self.encoder = encoder\n",
        "       self.decoder = decoder\n",
        "       self.device = device\n",
        "       self.mode = mode\n",
        "   def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "\n",
        "       input_length = source.size(0) #get the input length (number of words in sentence)\n",
        "       batch_size = target.shape[1] \n",
        "       target_length = target.shape[0]\n",
        "       vocab_size = self.decoder.output_dim\n",
        "      \n",
        "#initialize a variable to hold the predicted outputs\n",
        "       outputs = torch.zeros(target_length, batch_size, vocab_size).to(self.device)\n",
        "\n",
        "#encode every word in a sentence\n",
        "       for i in range(input_length):\n",
        "           encoder_output, encoder_hidden = self.encoder(source[i])\n",
        "\n",
        "#use the encoder’s hidden layer as the decoder hidden\n",
        "       \n",
        "       if self.mode == \"GRU\":\n",
        "         decoder_hidden = encoder_hidden.to(device)\n",
        "       elif self.mode == \"LSTM\":\n",
        "         decoder_hidden = (encoder_hidden[0].to(device),encoder_hidden[1].to(device))\n",
        "       \n",
        "#add a token before the first predicted word\n",
        "       decoder_input = torch.tensor([SOS_token], device=device)  # SOS\n",
        "\n",
        "#topk is used to get the top K value over a list\n",
        "#predict the output word from the current target word. If we enable the teaching force,  then the #next decoder input is the next word, else, use the decoder output highest value. \n",
        "\n",
        "       for t in range(target_length):   \n",
        "           decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "           outputs[t] = decoder_output\n",
        "           teacher_force = random.random() < teacher_forcing_ratio\n",
        "           topv, topi = decoder_output.topk(1)\n",
        "           input = (target[t] if teacher_force else topi)\n",
        "           if(teacher_force == False and input.item() == EOS_token):\n",
        "               break\n",
        "\n",
        "       return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Oj4Yc1UFuC-"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OJWRGTEDFuC-"
      },
      "outputs": [],
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "metric = datasets.load_metric('sacrebleu')\n",
        "\n",
        "\n",
        "def clacModel(model, input_tensor, target_tensor, model_optimizer, criterion):\n",
        "\n",
        "   model_optimizer.zero_grad()\n",
        "\n",
        "   input_length = input_tensor.size(0)\n",
        "   loss = 0\n",
        "   epoch_loss = 0\n",
        " \n",
        "\n",
        "   output = model(input_tensor, target_tensor,teacher_forcing_ratio)\n",
        "\n",
        "   num_iter = output.size(0)\n",
        "\n",
        "\n",
        "   for ot in range(num_iter):\n",
        "       loss += criterion(output[ot], target_tensor[ot])\n",
        "       \n",
        "   metric.add(predictions=output, references=target_tensor)\n",
        "   \n",
        "   loss.backward()\n",
        "   model_optimizer.step()\n",
        "   epoch_loss = loss.item() / num_iter\n",
        "\n",
        "   return epoch_loss,metric\n",
        "\n",
        "def trainModel(model, source, target, pairs, num_iteration=20000,type='GRU'):\n",
        "   \n",
        "   model.train()\n",
        "\n",
        "   optimizer = torch.optim.Adam(model.parameters(), lr=0.01,weight_decay=1e-5)\n",
        " \n",
        "   criterion = nn.CrossEntropyLoss()\n",
        "   total_loss_iterations = 0\n",
        "   final_score = 0\n",
        "   training_pairs = [tensorsFromPair(source, target, random.choice(pairs))\n",
        "                     for i in range(num_iteration)]\n",
        "  \n",
        "   for iter in tqdm(range(1, num_iteration+1)):\n",
        "       training_pair = training_pairs[iter - 1]\n",
        "       input_tensor = training_pair[0]\n",
        "       target_tensor = training_pair[1]\n",
        "\n",
        "       loss,metric = clacModel(model, input_tensor, target_tensor, optimizer, criterion)\n",
        "       s  = metric.compute()\n",
        "\n",
        "       final_score += s['score']\n",
        "       \n",
        "       total_loss_iterations += loss\n",
        "\n",
        "       if iter % 5000 == 0:\n",
        "           avarage_loss= total_loss_iterations / 5000\n",
        "           avg_score = final_score /5000\n",
        "           total_loss_iterations = 0\n",
        "           tqdm.write(f'iter :{iter} , Loss : {avarage_loss} , score :{avg_score}')\n",
        "          #  scheduler.step()\n",
        "           torch.save(model.state_dict(), f'/content/drive/MyDrive/{type}_Model.pt')\n",
        "          \n",
        "   return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm6otv8sFuC_"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Gabw_5sDFuC_"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, input_lang, output_lang, sentences, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentences[0])\n",
        "        output_tensor = tensorFromSentence(output_lang, sentences[1])\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        output = model(input_tensor, output_tensor)\n",
        "\n",
        "        for ot in range(output.size(0)):\n",
        "            topv, topi = output[ot].topk(1)\n",
        "           \n",
        "            if topi[0].item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi[0].item()])\n",
        "    return decoded_words\n",
        "\n",
        "def evaluateRandomly(model, source, target, pairs, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('source {}'.format(pair[0]))\n",
        "        print('target {}'.format(pair[1]))\n",
        "        output_words = evaluate(model, source, target, pair)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('predicted {}'.format(output_sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "fRwRoy7Cf7eT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lang1 = 'M1'\n",
        "lang2 = 'M2'\n",
        "source, target, pairs = process_data(lang1, lang2)\n",
        "\n",
        "randomize = random.choice(pairs)\n",
        "print('random sentence {}'.format(randomize))\n",
        "\n",
        "\n",
        "input_size = source.n_words\n",
        "output_size = target.n_words\n",
        "print('Input : {} Output : {}'.format(input_size, output_size))\n",
        "\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_layers = 1\n",
        "num_iteration = 50000\n",
        "\n",
        "\n",
        "encoder = Encoder(input_size, hidden_size, embed_size, num_layers,GRU=False)\n",
        "decoder = Decoder(output_size, hidden_size, embed_size, num_layers,GRU=False)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, device,mode='LSTM').to(device)\n",
        "\n",
        "\n",
        "print(encoder)\n",
        "print(decoder)\n",
        "\n",
        "model = trainModel(model, source, target, pairs, num_iteration,type='LSTM')\n",
        "evaluateRandomly(model, source, target, pairs)"
      ],
      "metadata": {
        "id": "j2w9YXZsaZ_j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "802d2534-3e66-42dc-c396-5195ee6d2602"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 49608 sentence pairs\n",
            "Maximum lenght of input is 11\n",
            "random sentence ['یکی باد برخاست از رزمگاه', 'هوا را بپوشید گرد سپاه']\n",
            "Input : 12687 Output : 13323\n",
            "Encoder(\n",
            "  (embedding): Embedding(12687, 256, padding_idx=2)\n",
            "  (recurrent): LSTM(256, 512)\n",
            ")\n",
            "Decoder(\n",
            "  (embedding): Embedding(13323, 256, padding_idx=2)\n",
            "  (recurrent): LSTM(256, 512)\n",
            "  (out): Linear(in_features=512, out_features=13323, bias=True)\n",
            "  (softmax): LogSoftmax(dim=1)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 5001/50000 [03:15<39:59, 18.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :5000 , Loss : 4.979427995522815 , score :0.3447447320598481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|█▉        | 9999/50000 [06:23<26:48, 24.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :10000 , Loss : 6.114715136496209 , score :0.7534923378830286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 15000/50000 [09:33<31:01, 18.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :15000 , Loss : 6.224572989082341 , score :1.2691679900365842\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 20000/50000 [12:42<27:34, 18.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :20000 , Loss : 6.192379944547011 , score :1.7849226412382098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 25000/50000 [15:51<17:58, 23.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :25000 , Loss : 6.368459616152446 , score :2.300480952284484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|█████▉    | 29998/50000 [19:00<11:58, 27.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :30000 , Loss : 6.970026843833915 , score :2.8161819743894725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|██████▉   | 34997/50000 [22:07<10:49, 23.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :35000 , Loss : 7.400589222017928 , score :3.3317307001659167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|███████▉  | 39998/50000 [25:15<05:50, 28.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :40000 , Loss : 7.725218291091918 , score :3.847343872948575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|████████▉ | 44999/50000 [28:22<03:36, 23.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :45000 , Loss : 7.6029853504180736 , score :4.3608444151926475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [31:29<00:00, 26.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :50000 , Loss : 7.504036335118614 , score :4.876393700286357\n",
            "source همان چون بخوردند از کاسه شیر\n",
            "target توگویی بخستند هر دو به تیر\n",
            "predicted همی دشت بشتافتی بشتافتی PAD PAD PAD PAD PAD PAD PAD PAD\n",
            "source چو این دو سرافگنده شد در نبرد\n",
            "target شماساس شد بی دل و روی زرد\n",
            "predicted همی دشت بشتافتی بشتافتی PAD PAD PAD PAD PAD PAD PAD PAD\n",
            "source بفرمود تا رستم آمد بتخت\n",
            "target نشست از بر گاه زیر درخت\n",
            "predicted همی دشت بشتافتی بشتافتی PAD PAD PAD PAD PAD PAD PAD PAD\n",
            "source بگاهی که تنها بود در نهفت\n",
            "target پشیمان شود زان سخنها که گفت\n",
            "predicted همی دشت بشتافتی بشتافتی PAD PAD PAD PAD PAD PAD PAD PAD\n",
            "source به نوروز چون برنشستی به تخت\n",
            "target به نزدیک او موبد نیک بخت\n",
            "predicted همی دشت بشتافتی بشتافتی PAD PAD PAD PAD PAD PAD PAD PAD\n",
            "source نه موبد بود شاد و نه پهلوان\n",
            "target نه او در جهان شاد روشن روان\n",
            "predicted همی دشت بشتافتی بشتافتی PAD PAD PAD PAD PAD PAD PAD PAD\n",
            "source چو فرمان دهد ما همیدون کنیم\n",
            "target زمین را ز خون رود جیحون کنیم\n",
            "predicted همی دشت بشتافتی بشتافتی PAD PAD PAD PAD PAD PAD PAD PAD\n",
            "source بران سال کودک برافراخت یال\n",
            "target بر شاه کابل فرستاد زال\n",
            "predicted همی دشت بشتافتی بشتافتی PAD PAD PAD PAD PAD PAD PAD PAD\n",
            "source پر اندیشه بنشست لهراسپ دیر\n",
            "target بفرمود تا پیش او شد زریر\n",
            "predicted همی دشت بشتافتی بشتافتی PAD PAD PAD PAD PAD PAD PAD PAD\n",
            "source مرین کوه را گنگ دژ در میان\n",
            "target بدان کت ز دانش نیاید زیان\n",
            "predicted همی دشت بشتافتی بشتافتی PAD PAD PAD PAD PAD PAD PAD PAD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xky23N_RFuDA"
      },
      "source": [
        "#GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNrsWxs_FuDA",
        "outputId": "a9eb9a12-1271-4409-b8bf-a83a6bbd2f4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 49608 sentence pairs\n",
            "Maximum lenght of input is 11\n",
            "random sentence ['همان روز ازان مرز لشکر براند', 'ورا بسته در پوست آنجا بماند']\n",
            "Input : 12687 Output : 13323\n",
            "Encoder(\n",
            "  (embedding): Embedding(12687, 256, padding_idx=2)\n",
            "  (recurrent): GRU(256, 512)\n",
            ")\n",
            "Decoder(\n",
            "  (embedding): Embedding(13323, 256, padding_idx=2)\n",
            "  (recurrent): GRU(256, 512)\n",
            "  (out): Linear(in_features=512, out_features=13323, bias=True)\n",
            "  (softmax): LogSoftmax(dim=1)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|▉         | 4999/50000 [03:00<30:50, 24.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :5000 , Loss : 5.441672592512755 , score :0.35861098150202986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|█▉        | 9997/50000 [05:59<27:40, 24.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :10000 , Loss : 5.355617233530666 , score :0.7034460254127358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|██▉       | 14999/50000 [09:00<24:02, 24.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :15000 , Loss : 6.229163198598232 , score :1.0806767369509904\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|███▉      | 19999/50000 [11:59<20:48, 24.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :20000 , Loss : 6.900528274663298 , score :1.5585917302280576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|████▉     | 24997/50000 [14:59<17:58, 23.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :25000 , Loss : 7.007371568806971 , score :2.0734969203527327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|█████▉    | 29999/50000 [17:58<13:41, 24.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :30000 , Loss : 6.6814912516911935 , score :2.588333845263808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|██████▉   | 34998/50000 [20:59<10:22, 24.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :35000 , Loss : 6.885610501289387 , score :3.103769080773882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|███████▉  | 39999/50000 [24:00<07:03, 23.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :40000 , Loss : 6.973181796646123 , score :3.6188872110850316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|████████▉ | 44999/50000 [27:02<02:55, 28.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :45000 , Loss : 7.51729236164093 , score :4.134174072441573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [30:05<00:00, 27.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :50000 , Loss : 7.444456123701708 , score :4.648357564452722\n",
            "source بدان تا روان تو روشن کند\n",
            "target خرد پیش مغز تو جوشن کند\n",
            "predicted بسی کژی کژی کژی PAD PAD PAD PAD PAD PAD PAD PAD\n",
            "source همی رفت گودرز با شهریار\n",
            "target چو آمد بدان گلشن زرنگار\n",
            "predicted بسی کژی کژی کژی PAD PAD PAD PAD PAD PAD PAD PAD\n",
            "source زمین چار فرسنگ بالای اوی\n",
            "target برین هم نشانست پهنای اوی\n",
            "predicted بسی کژی کژی کژی PAD PAD PAD PAD PAD PAD PAD PAD\n",
            "source بر این گونه شد سنگ در پیش من\n",
            "target نبود آگه از رای کم بیش من\n",
            "predicted بسی کژی کژی کژی PAD PAD PAD PAD PAD PAD PAD PAD\n",
            "source وگر خاست اندر جهان رستخیز\n",
            "target که بفروختی آتش مهر تیز\n",
            "predicted بسی کژی کژی کژی PAD PAD PAD PAD PAD PAD PAD PAD\n",
            "source بیاراست دانا یکی رزمگاه\n",
            "target به قلب اندرون ساخته جای شاه\n",
            "predicted بسی کژی کژی کژی PAD PAD PAD PAD PAD PAD PAD PAD\n",
            "source همی تاختش تا بدیشان رسید\n",
            "target سر جاودان چون مر او را بدید\n",
            "predicted بسی کژی کژی کژی PAD PAD PAD PAD PAD PAD PAD PAD\n",
            "source دگر آنک بر گرگ بدرید پوست\n",
            "target همه روم یکسر پرآواز اوست\n",
            "predicted بسی کژی کژی کژی PAD PAD PAD PAD PAD PAD PAD PAD\n",
            "source منوچهر چون یافت زو آگهی\n",
            "target بیاراست دیهیم شاهنشهی\n",
            "predicted بسی کژی کژی کژی PAD PAD PAD PAD PAD PAD PAD PAD\n",
            "source کنون ای پسر گاه آرایشست\n",
            "target نه هنگام پیری و بخشایشست\n",
            "predicted بسی کژی کژی کژی PAD PAD PAD PAD PAD PAD PAD PAD\n"
          ]
        }
      ],
      "source": [
        "lang1 = 'M1'\n",
        "lang2 = 'M2'\n",
        "source, target, pairs = process_data(lang1, lang2)\n",
        "\n",
        "randomize = random.choice(pairs)\n",
        "print('random sentence {}'.format(randomize))\n",
        "\n",
        "\n",
        "\n",
        "input_size = source.n_words\n",
        "output_size = target.n_words\n",
        "print('Input : {} Output : {}'.format(input_size, output_size))\n",
        "\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_layers = 1\n",
        "num_iteration = 50000\n",
        "\n",
        "\n",
        "encoder = Encoder(input_size, hidden_size, embed_size, num_layers,GRU=True)\n",
        "decoder = Decoder(output_size, hidden_size, embed_size, num_layers,GRU=True)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, device,mode='GRU').to(device)\n",
        "\n",
        "\n",
        "print(encoder)\n",
        "print(decoder)\n",
        "\n",
        "model = trainModel(model, source, target, pairs, num_iteration,type='GRU')\n",
        "evaluateRandomly(model, source, target, pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "reference: https://www.guru99.com/seq2seq-model.html"
      ],
      "metadata": {
        "id": "HyiPfy7vPnBO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IZo3aBTogDnu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}